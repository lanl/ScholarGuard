
## Build

`$> docker-compose build`

## Start

`$> docker-compose up`

## Stop

`<CTRL+C>`

Or

`$> docker-compose down`


# Developement Environment

When developing and testing the core orchestrator in a local machine, the docker containers 
can be built and run in a desktop replicating the production env in the server. However, running 
`docker-compose build/stop/start` for every little change in development can become very time-consuming. 
Hence, a simpler dev environment can be setup locally that would speed things up. This involves not starting 
the core orchestrator app from docker and only starting the databases as docker containers. 

The steps involved to set up this dev environment are:

* Clone the repository.
    * `$> cd <work/projects folder>`
	* `$> cd orchestrator`
* Comment out all the lines corresponding to the orchestrator app in the `docker-compose.yml` file. The 
  orchestrator app service is called `pod-app` in this file and that entire section will have to be commented out. 
    * Depending on the feature being developed, it is possible that the celery components are also not needed 
      during development. These can also be commented out. This simplifies the setup futher. 
	* Typically, a copy of the docker-compose file for production can be made and these changes can be 
	  applied on the copy.
* Copy the data from the live system to the local dev environment.
    * The elasticsearch and sqlite data is in the fodler `/data/web/ScholarGuard/orchestrator/data` in the live server. 
	  This folder can be zipped/tarred and scp'ed over to the local machine.
	  `tar cvzf orc_data_20190711.tar.gz /data/web/ScholarGuard/orchestrator/data` is the command I typically use to tar. 
	* Both elasticsearch and sqlite will pick up the data and the indexes in the local machine without 
	  having to re-index. 
	* This tarred data can be copied over to the local orchestrator directory and extracted in the `data` folder.
* Start the databases. 
    * `$> docker-compose build`
	* `$> docker-compose up`
* Check the configuration file `./conf/config-standalone.yaml`. 
    * Make sure the properties `pod:server_name`, `pod:preferred_url_scheme`, `db:sqlalchemy_database_uri`, 
	  `db:es:host`, and `db:es:port` are correct. 
	* Make sure the permissions of the `./data/es` and `./data/sql` folders are correct. 
* Start the orchestrator app
    * Creating a python virtualenv and install the orchestrator app in that.
	  `$> source ~/venv/orchestrator/bin/activate` and `$> cd web && python3 setup.py install`.
    * The convenience file `./dev_server.sh` sets all the appropriate environment variables 
	  and starts the app. The command `flask run` is started in debug mode, hence any changes made to the 
	  code should be automatically detected and flask will restart it's server without any user intervention.
	* The script `./test_get_events.sh` also provides convenience methods to test various functions in 
	  development.
* Open the orchestrator in a browser.
    * The URL to open the orchestrator UI is the value of `pod:server_name` in `./conf/config-standalone.yaml`.
	* If this url is not recognized by the browser, usually when trying this for the first 
	  time, then please add the value of `pod:server_name` to the hosts file.
	  After adding the server_name `pod_add` to the hosts file `$> sudo vim /etc/hosts`, 
	  the corresponding line in the this file looks like `127.0.0.1	localhost pod-app`.
	  
# AS2 Models

The AS2 models for various portals are available in the [models folder](./models).
Each portal in the models folder is split into three directories: `tracker`, `capture`, and `archiver`.
Each of these directories contain models for the `tracker`, `capture`, and `archiver` components of the pod.
Under each of these directories, there are AS2 message models for the most common event types for a portal.

All the models are serialized in valid JSON-LD.
They are passed around and are consumed directly by the various components in the pod.
In all the models across various portals, the researcher the pod is tracking is named `Alice`, and only the artifacts `Alice` creates and interacts with is of interest for the pod.
Hence, artifacts created by other researchers, even when they may be in a portal that `Alice` is involved in, is not of concern for the pod.
For example, if a researcher named `Bob` commits a contribution to a repository owned by `Alice`, `Alice's` pod will not track this event.
Also, for simplicity, the models assume that `Alice` only interacts with artifacts created by `Bob`.

The AS2 messages generated by the tracker and the archiver must be based on these models.
To add a new model or to edit an existing model, the following steps must be taken to make sure the models are compatible:

* The JSON must be converted to RDF/XML using [http://www.easyrdf.org/converter](http://www.easyrdf.org/converter).
* The RDF/XML output from the previous step is validated and visualized using:
[https://www.w3.org/RDF/Validator/](https://www.w3.org/RDF/Validator/).

The RDF conversion and visualization helps keep the model and the graph simple as things can become very
complex very fast when working with JSON alone.
All of the existing models have been validated by performing the steps above, so any existing model can be used for comparison when creating any new model.

